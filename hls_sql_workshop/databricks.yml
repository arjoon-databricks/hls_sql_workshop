# This is a Databricks asset bundle definition for hls_sql_workshop.
# See https://docs.databricks.com/dev-tools/bundles/index.html for documentation.
bundle:
  name: hls_sql_workshop

include:
  - resources/*.yml

# Define variables across environments
variables:
  development_mode:
    description: The development mode for DLT
    default: true
  cluster_workers:
    description: The numer of workers the cluster should use
    default: 2
  catalog:
    description: The target environment to add to the catalog name
    default: ddavis_demo
  schema: 
    description: The target schema for bronze, silver, and gold data
    default: cms
  volume: 
    description: The volume to create to load raw data files into
    default: raw_files    
  
targets:
  # The 'dev' target, for development purposes. This target is the default.
  dev:
    # We use 'mode: development' to indicate this is a personal development copy:
    # - Deployed resources get prefixed with '[dev my_user_name]'
    # - Any job schedules and triggers are paused by default
    # - The 'development' mode is used for Delta Live Tables pipelines
    mode: development
    default: true
    workspace:
      host: https://e2-demo-field-eng.cloud.databricks.com
    resources:
      pipelines:
        hls_sql_workshop_pipeline:
          serverless: true


  # The 'aws' target, used for production deployment.
  aws-classic-compute:
    # We use 'mode: production' to indicate this is a production deployment.
    # Doing so enables strict verification of the settings below.
    mode: development
    #workspace:
      #host: https://e2-demo-field-eng.cloud.databricks.com
      # We always use /Users/dan.davis@databricks.com for all resources to make sure we only have a single copy.
      # If this path results in an error, please make sure you have a recent version of the CLI installed.
      #root_path: /Users/${workspace.current_user.userName}/.bundle/${bundle.name}/${bundle.target}
    resources:
      ## add an environment override for the jobs notifications
      jobs: 
        hls_sql_workshop_setup:
          job_clusters:
          - job_cluster_key: ml_job_cluster
            new_cluster:
              cluster_name: ""
              spark_version: 15.4.x-cpu-ml-scala2.12
              aws_attributes:
                first_on_demand: 1
                availability: SPOT_WITH_FALLBACK
                spot_bid_price_percent: 100
                ebs_volume_count: 0
              node_type_id: r6id.xlarge
              spark_env_vars:
                PYSPARK_PYTHON: /databricks/python3/bin/python3
              enable_elastic_disk: false
              data_security_mode: SINGLE_USER
              runtime_engine: PHOTON
              num_workers: 4
          - job_cluster_key: ml_job_cluster
            new_cluster:
              cluster_name: ""
              spark_version: 15.4.x-scala2.12
              aws_attributes:
                first_on_demand: 1
                availability: SPOT_WITH_FALLBACK
                spot_bid_price_percent: 100
                ebs_volume_count: 0
              node_type_id: r6id.xlarge
              spark_env_vars:
                PYSPARK_PYTHON: /databricks/python3/bin/python3
              enable_elastic_disk: false
              data_security_mode: SINGLE_USER
              runtime_engine: PHOTON
              num_workers: 4           
      pipelines:
        hls_sql_workshop_pipeline:
          clusters:
          - label: default
            aws_attributes:
              availability: ON_DEMAND
            autoscale:
              min_workers: 1
              max_workers: 4
              mode: ENHANCED                  

  # The 'azure' target, used for production deployment.
  azure-classic-compute:
    # We use 'mode: production' to indicate this is a production deployment.
    # Doing so enables strict verification of the settings below.
    mode: development
    #workspace:
      #host: https://e2-demo-field-eng.cloud.databricks.com
      # We always use /Users/dan.davis@databricks.com for all resources to make sure we only have a single copy.
      # If this path results in an error, please make sure you have a recent version of the CLI installed.
      #root_path: /Users/${workspace.current_user.userName}/.bundle/${bundle.name}/${bundle.target}
    resources:
      ## add an environment override for the jobs notifications
      jobs: 
        hls_sql_workshop_setup:
          job_clusters:
          - job_cluster_key: ml_job_cluster
            new_cluster:
              cluster_name: ""
              spark_version: 15.4.x-cpu-ml-scala2.12
              azure_attributes:
                first_on_demand: 1
                availability: ON_DEMAND_AZURE
                spot_bid_max_price: -1
              node_type_id: Standard_D4ds_v5
              spark_env_vars:
                PYSPARK_PYTHON: /databricks/python3/bin/python3
              enable_elastic_disk: true
              data_security_mode: SINGLE_USER
              runtime_engine: PHOTON
              num_workers: 4
          - job_cluster_key: dbr_job_cluster
            new_cluster:
              cluster_name: ""
              spark_version: 15.4.x-scala2.12
              azure_attributes:
                first_on_demand: 1
                availability: ON_DEMAND_AZURE
                spot_bid_max_price: -1
              node_type_id: Standard_D4ds_v5
              spark_env_vars:
                PYSPARK_PYTHON: /databricks/python3/bin/python3
              enable_elastic_disk: true
              data_security_mode: SINGLE_USER
              runtime_engine: PHOTON
              num_workers: 4
      pipelines:
        hls_sql_workshop_pipeline:
          clusters:
          - label: default
            autoscale:
              min_workers: 1
              max_workers: 4
              mode: ENHANCED                                  
