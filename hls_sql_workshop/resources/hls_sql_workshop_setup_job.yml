resources:
  jobs:
    hls_sql_workshop_setup:
      name: hls_sql_workshop_setup
      tasks:
        - task_key: uc_setup
          notebook_task:
            notebook_path: ../src/notebooks/setup/uc_setup.py
            base_parameters:
              schema: ${var.schema}
              volume: ${var.volume}
            source: WORKSPACE
        #  job_cluster_key: dbr_job_cluster             
        - task_key: copy_files_to_volume
          depends_on:
            - task_key: uc_setup       
          notebook_task:
            notebook_path: ../src/notebooks/setup/copy_files_to_volume.py
            base_parameters:
              schema: ${var.schema}
              volume: ${var.volume}
            source: WORKSPACE
        #  job_cluster_key: dbr_job_cluster                        
        - task_key: dlt_etl
          depends_on:
            - task_key: copy_files_to_volume
          pipeline_task:
            pipeline_id: ${resources.pipelines.hls_sql_workshop_pipeline.id}
            full_refresh: false
        - task_key: copy_gold_tables_add_metadata
          depends_on:
            - task_key: dlt_etl
          notebook_task:
            notebook_path: ../src/notebooks/setup/gold_copy_tables_add_metadata.py
            base_parameters:
              schema: ${var.schema}
            source: WORKSPACE
        #  job_cluster_key: dbr_job_cluster             
        - task_key: build_feature_store_beneficiary
          depends_on:
            - task_key: copy_gold_tables_add_metadata
          notebook_task:
            notebook_path: ../src/notebooks/setup/ml/01_build_training_dataset.py
            source: WORKSPACE
        #  job_cluster_key: dbr_job_cluster             
        - task_key: ml_train_and_register_model 
          depends_on:
            - task_key: build_feature_store_beneficiary
          notebook_task:
            notebook_path: ../src/notebooks/setup/ml/02_ml_train_and_register_model.py
            source: WORKSPACE
        #  job_cluster_key: ml_job_cluster  
        - task_key: create_online_table
          depends_on:
            - task_key: ml_train_and_register_model 
          notebook_task:
            notebook_path: ../src/notebooks/setup/ml/03_create_online_table.py
            source: WORKSPACE
        #  job_cluster_key: ml_job_cluster            
        - task_key: create_serving_endpoint
          depends_on:
            - task_key: create_online_table
          notebook_task:
            notebook_path: ../src/notebooks/setup/ml/04_create_serving_endpoint.py
            source: WORKSPACE
         # job_cluster_key: ml_job_cluster  
      # job_clusters:
      #   - job_cluster_key: ml_job_cluster
      #     new_cluster:
      #       spark_version: 15.4.x-cpu-ml-scala2.12
      #       spark_conf:
      #         spark.master: local[*, 4]
      #       aws_attributes:
      #         first_on_demand: 1
      #         availability: SPOT_WITH_FALLBACK
      #         spot_bid_price_percent: 100
      #         ebs_volume_count: 0
      #       node_type_id: r6id.xlarge
      #       enable_elastic_disk: true
      #       data_security_mode: SINGLE_USER
      #       runtime_engine: STANDARD
      #       num_workers: 4
      #   - job_cluster_key: dbr_job_cluster
      #     new_cluster:
      #       cluster_name: ""
      #       spark_version: 15.4.x-scala2.12
      #       azure_attributes:
      #         first_on_demand: 1
      #         availability: ON_DEMAND_AZURE
      #         spot_bid_max_price: -1
      #       node_type_id: Standard_D4ds_v5
      #       spark_env_vars:
      #         PYSPARK_PYTHON: /databricks/python3/bin/python3
      #       enable_elastic_disk: true
      #       data_security_mode: SINGLE_USER
      #       runtime_engine: PHOTON
      #       num_workers: 4              
      queue:
        enabled: true
      tags:
        industry: hls
        purpose: sql_workshop
      parameters:
        - name: catalog
          default: ${var.catalog}